{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/johir-bd/Machine-Learning-Project/blob/master/Q_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import time\n",
        "\n",
        "class SelfDrivingCar:\n",
        "    def __init__(self):\n",
        "        self.speed = 0  # in km/h\n",
        "        self.distance_to_obstacle = 100  # distance to nearest obstacle in meters\n",
        "\n",
        "    def sense_environment(self):\n",
        "        \"\"\" Simulate sensing the environment \"\"\"\n",
        "        # Randomly change distance to obstacle\n",
        "        self.distance_to_obstacle = random.randint(20, 100)\n",
        "\n",
        "    def decide_speed(self):\n",
        "        \"\"\" Basic decision-making: adjust speed based on obstacle distance \"\"\"\n",
        "        if self.distance_to_obstacle < 30:\n",
        "            self.speed = 0  # Stop if too close to an obstacle\n",
        "            return \"Obstacle detected! Stopping the car.\"\n",
        "        elif 30 <= self.distance_to_obstacle < 60:\n",
        "            self.speed = 30  # Slow down\n",
        "            return \"Obstacle ahead! Slowing down.\"\n",
        "        else:\n",
        "            self.speed = 60  # Drive at normal speed\n",
        "            return \"All clear! Driving at normal speed.\"\n",
        "\n",
        "    def drive(self, iterations=10):\n",
        "        \"\"\" Simulate driving decision-making for a number of iterations \"\"\"\n",
        "        output = []\n",
        "        for _ in range(iterations):\n",
        "            self.sense_environment()\n",
        "            decision = self.decide_speed()\n",
        "            output.append(f\"Current speed: {self.speed} km/h, Distance to obstacle: {self.distance_to_obstacle} meters. {decision}\")\n",
        "            time.sleep(0.5)  # Slow down output for readability\n",
        "        return output\n",
        "\n",
        "# Run the simulation\n",
        "car = SelfDrivingCar()\n",
        "results = car.drive(iterations=10)\n",
        "\n",
        "# Print results\n",
        "for result in results:\n",
        "    print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r48Xk06HETw6",
        "outputId": "a182ce99-7eb1-4949-9d9e-5178791f5125"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current speed: 60 km/h, Distance to obstacle: 78 meters. All clear! Driving at normal speed.\n",
            "Current speed: 30 km/h, Distance to obstacle: 37 meters. Obstacle ahead! Slowing down.\n",
            "Current speed: 60 km/h, Distance to obstacle: 96 meters. All clear! Driving at normal speed.\n",
            "Current speed: 60 km/h, Distance to obstacle: 96 meters. All clear! Driving at normal speed.\n",
            "Current speed: 60 km/h, Distance to obstacle: 76 meters. All clear! Driving at normal speed.\n",
            "Current speed: 30 km/h, Distance to obstacle: 32 meters. Obstacle ahead! Slowing down.\n",
            "Current speed: 60 km/h, Distance to obstacle: 80 meters. All clear! Driving at normal speed.\n",
            "Current speed: 60 km/h, Distance to obstacle: 86 meters. All clear! Driving at normal speed.\n",
            "Current speed: 30 km/h, Distance to obstacle: 58 meters. Obstacle ahead! Slowing down.\n",
            "Current speed: 60 km/h, Distance to obstacle: 96 meters. All clear! Driving at normal speed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gym\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bas9dw8mKKCV",
        "outputId": "c7acbbab-6a68-4514-aea6-23fb2751d8b7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gym in /usr/local/lib/python3.10/dist-packages (0.25.2)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from gym) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym) (3.1.0)\n",
            "Requirement already satisfied: gym_notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym) (0.0.8)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import gym\n",
        "\n",
        "# Create a simple environment, here CartPole is used as an example\n",
        "env = gym.make('CartPole-v1')\n",
        "env.reset()\n",
        "\n",
        "# Run a simple episode\n",
        "for _ in range(1000):\n",
        "    env.render()  # This usually displays the environment, not usable in Colab\n",
        "    action = env.action_space.sample()  # Take a random action\n",
        "    env.step(action)  # Execute the action\n",
        "\n",
        "env.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Xakz6dcKG-t",
        "outputId": "52fce6bc-a51f-4e5f-c409-4002f8d3a2a5"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:49: DeprecationWarning: \u001b[33mWARN: You are calling render method, but you didn't specified the argument render_mode at environment initialization. To maintain backward compatibility, the environment will render in human mode.\n",
            "If you want to render in human mode, initialize the environment in this way: gym.make('EnvName', render_mode='human') and don't call the render method.\n",
            "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/pygame/pkgdata.py:25: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n",
            "  from pkg_resources import resource_stream, resource_exists\n",
            "/usr/local/lib/python3.10/dist-packages/pkg_resources/__init__.py:3154: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.\n",
            "Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
            "  declare_namespace(pkg)\n",
            "/usr/local/lib/python3.10/dist-packages/pkg_resources/__init__.py:3154: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google.cloud')`.\n",
            "Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
            "  declare_namespace(pkg)\n",
            "/usr/local/lib/python3.10/dist-packages/pkg_resources/__init__.py:3154: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('sphinxcontrib')`.\n",
            "Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
            "  declare_namespace(pkg)\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:241: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
            "  if not isinstance(terminated, (bool, np.bool8)):\n",
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:49: DeprecationWarning: \u001b[33mWARN: You are calling render method, but you didn't specified the argument render_mode at environment initialization. To maintain backward compatibility, the environment will render in human mode.\n",
            "If you want to render in human mode, initialize the environment in this way: gym.make('EnvName', render_mode='human') and don't call the render method.\n",
            "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/envs/classic_control/cartpole.py:179: UserWarning: \u001b[33mWARN: You are calling 'step()' even though this environment has already returned terminated = True. You should always call 'reset()' once you receive 'terminated = True' -- any further steps are undefined behavior.\u001b[0m\n",
            "  logger.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "\n",
        "# Create the CartPole environment\n",
        "env = gym.make('CartPole-v1')\n",
        "\n",
        "# Observe the initial state\n",
        "state = env.reset()\n",
        "print(\"Initial state:\", state)\n",
        "\n",
        "# Close the environment\n",
        "env.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eG8TVcf0K-O-",
        "outputId": "13fe0a43-ef4c-498f-e99d-b3e39a5b682e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial state: [-0.02160355 -0.0331586  -0.0092422   0.0469237 ]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Number of episodes to run\n",
        "num_episodes = 5\n",
        "\n",
        "for episode in range(num_episodes):\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    total_reward = 0\n",
        "\n",
        "    while not done:\n",
        "        # Render the environment\n",
        "        env.render()\n",
        "\n",
        "        # Choose a random action\n",
        "        action = env.action_space.sample()\n",
        "\n",
        "        # Step the environment using the action\n",
        "        state, reward, done, info = env.step(action)\n",
        "        total_reward += reward\n",
        "\n",
        "    print(f\"Episode {episode + 1}: Total Reward: {total_reward}\")\n",
        "\n",
        "# Close the environment\n",
        "env.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T1S5SLKRLGP4",
        "outputId": "56b9ca1b-3bb5-409b-8d7c-dedaadfc2c07"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 1: Total Reward: 44.0\n",
            "Episode 2: Total Reward: 25.0\n",
            "Episode 3: Total Reward: 15.0\n",
            "Episode 4: Total Reward: 26.0\n",
            "Episode 5: Total Reward: 26.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import gym\n",
        "\n",
        "# Create the CartPole environment\n",
        "env = gym.make('CartPole-v1')\n",
        "\n",
        "# Parameters\n",
        "num_episodes = 1000\n",
        "learning_rate = 0.1\n",
        "discount_factor = 0.95\n",
        "exploration_prob = 1.0\n",
        "exploration_decay = 0.995\n",
        "min_exploration_prob = 0.01\n",
        "\n",
        "# Discretizing the state parameters\n",
        "def discretize_state(state):\n",
        "    # Define bins for each observation (4 observations in CartPole)\n",
        "    bins = [\n",
        "        np.linspace(-2.4, 2.4, 10),  # Position of the cart (x)\n",
        "        np.linspace(-3.0, 3.0, 10),  # Velocity of the cart (x')\n",
        "        np.linspace(-0.5, 0.5, 10),  # Angle of the pole (theta)\n",
        "        np.linspace(-2.0, 2.0, 10)   # Velocity of the pole (theta')\n",
        "    ]\n",
        "    # Digitize the state into bin indices, corrected to fit Q-table dimensions\n",
        "    discretized = []\n",
        "    for i in range(len(state)):\n",
        "        index = np.digitize(state[i], bins[i])\n",
        "        # Ensure the index fits in the Q-table dimension\n",
        "        index = min(index, len(bins[i]) - 1)  # cap the index to prevent overflow\n",
        "        discretized.append(index)\n",
        "    return tuple(discretized)\n",
        "\n",
        "# Size of Q-table based on the number of bins across all dimensions\n",
        "q_table = np.zeros((10, 10, 10, 10, env.action_space.n))\n",
        "\n",
        "# Train the agent\n",
        "for episode in range(num_episodes):\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    total_reward = 0\n",
        "\n",
        "    while not done:\n",
        "        # Choose action (exploration or exploitation)\n",
        "        if np.random.rand() < exploration_prob:\n",
        "            action = env.action_space.sample()  # Explore\n",
        "        else:\n",
        "            # Exploit: select the action with max Q value\n",
        "            action = np.argmax(q_table[discretize_state(state)])\n",
        "\n",
        "        # Step the environment\n",
        "        new_state, reward, done, info = env.step(action)\n",
        "        total_reward += reward\n",
        "\n",
        "        # Update Q-value\n",
        "        best_future_q = np.max(q_table[discretize_state(new_state)])\n",
        "        q_table[discretize_state(state)][action] += learning_rate * (reward + discount_factor * best_future_q - q_table[discretize_state(state)][action])\n",
        "\n",
        "        state = new_state\n",
        "\n",
        "    # Decay exploration probability\n",
        "    exploration_prob = max(min_exploration_prob, exploration_prob * exploration_decay)\n",
        "\n",
        "    if episode % 100 == 0:\n",
        "        print(f\"Episode {episode}: Total Reward: {total_reward}, Exploration Probability: {exploration_prob:.2f}\")\n",
        "\n",
        "# Close the environment\n",
        "env.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NVqQiqDNLQsA",
        "outputId": "887e10ce-8374-45d8-fb39-270daa8eae74"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 0: Total Reward: 12.0, Exploration Probability: 0.99\n",
            "Episode 100: Total Reward: 53.0, Exploration Probability: 0.60\n",
            "Episode 200: Total Reward: 20.0, Exploration Probability: 0.37\n",
            "Episode 300: Total Reward: 46.0, Exploration Probability: 0.22\n",
            "Episode 400: Total Reward: 24.0, Exploration Probability: 0.13\n",
            "Episode 500: Total Reward: 29.0, Exploration Probability: 0.08\n",
            "Episode 600: Total Reward: 45.0, Exploration Probability: 0.05\n",
            "Episode 700: Total Reward: 44.0, Exploration Probability: 0.03\n",
            "Episode 800: Total Reward: 34.0, Exploration Probability: 0.02\n",
            "Episode 900: Total Reward: 54.0, Exploration Probability: 0.01\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the agent\n",
        "for episode in range(5):\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    total_reward = 0\n",
        "\n",
        "    while not done:\n",
        "        env.render()\n",
        "        action = np.argmax(q_table[discretize_state(state)])  # Choose the best action\n",
        "        state, reward, done, info = env.step(action)\n",
        "        total_reward += reward\n",
        "\n",
        "    print(f\"Evaluation Episode {episode + 1}: Total Reward: {total_reward}\")\n",
        "\n",
        "# Close the environment\n",
        "env.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_PX81lJuMSp5",
        "outputId": "e98fa7fa-2374-438b-c272-5656cbe931f2"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation Episode 1: Total Reward: 62.0\n",
            "Evaluation Episode 2: Total Reward: 30.0\n",
            "Evaluation Episode 3: Total Reward: 29.0\n",
            "Evaluation Episode 4: Total Reward: 41.0\n",
            "Evaluation Episode 5: Total Reward: 45.0\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyOTWCFPOvIgqmOWfD4wpUzU",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}